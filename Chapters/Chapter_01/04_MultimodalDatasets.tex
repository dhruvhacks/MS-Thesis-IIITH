\section{Multimodal datasets for emotion recognition}
\label{sec:multimodalDatasets}
In the landscape of multimodal emotion datasets, several initiatives have aimed to capture the intricacies of human emotions across different contexts. In this section, we delve into key datasets relevant to our research focus, highlighting their distinctive features.

\paragraph{Acted Facial Expressions in the Wild (AFEW)~\cite{afew}}:
\label{sec:afew}
AFEW is designed to predict emotions solely from facial expressions, emphasizing the spontaneous nature of emotional reactions. However, it lacks contextual information, providing a limited scope for understanding emotions within the broader context of a narrative.

\paragraph{Stanford Emotional Narratives Dataset~\cite{ongSENDv1}}:
\label{sec:sendv1}
This dataset captures participant-shared narratives of positive and negative events in their lives. While multimodal in nature, incorporating both textual and visual elements, the narratives differ substantially from the edited and scripted content of movies and stories, which constitutes the primary focus of our research.

\paragraph{Multimodal EmotionLines Dataset (MELD)~\cite{poria2019meld}}:
\label{sec:meld}
MELD is an example of Emotion Recognition in Conversations (ERC), concentrating on estimating emotions for individual dialog utterances in TV episodes from the show \emph{Friends}. Differing from MELD, our research operates at the time-scale of cohesive story units, specifically movie scenes, allowing for a more comprehensive understanding of emotions within a narrative context.

\paragraph{Annotated Creative Commons Emotional DatabasE (LIRIS-ACCEDE)~\cite{BaveyeLIRIS}}:
\label{sec:lirisaccede}
LIRIS-ACCEDE provides emotion annotations for short movie clips, making it closely aligned with our focus on cinematic content. However, the clips in LIRIS-ACCEDE are relatively small (8-12 seconds), and annotations are obtained in the continuous valence-arousal space, offering a different perspective compared to our multi-label approach that includes both classic emotions and mental states.

\paragraph{MovieGraphs dataset~\cite{moviegraphs}}
MovieGraphs dataset features 51 movies and 7637 movie scenes with detailed graph annotations. Like other annotations in the MovieGraphs dataset, emotions are also obtained as free-text leading to a huge variability and a long-tail of labels (over 500).  It also includes annotations such as the situation label, or character interactions and relationships~\cite{lirec}.

In essence, the exploration of these multimodal emotion datasets illuminates the varied approaches undertaken in understanding emotional expressions.
