\section{Transformer Self-attention.}
\label{sec:transformer}

In the core architecture of EmoTx, the Transformer self-attention mechanism allows the fusion of information across visual, facial and language modalities for nuanced emotion recognition.

The process unfolds through the concatenation of all relevant embeddings to the feature vectors, representing different aspects of the input data, as they traverse through $H{=}2$ layers of the Transformer encoder~\cite{attention}. These layers are instrumental in facilitating self-attention, a mechanism that allows the model to weigh and prioritize different parts of the input sequence based on their contextual relevance. The self-attention mechanism is particularly powerful in capturing intricate relationships and dependencies across various modalities, enabling a holistic understanding of the complex information present in movie scenes.

Within the context of emotion prediction, our focus narrows down to specific outputs generated by the Transformer encoder. These outputs correspond exclusively to the classification tokens, which are strategically chosen to encapsulate the most salient information for the task at hand. By selectively tapping into these outputs, EmoTx hones in on the essential elements relevant to predicting emotions, optimizing the model's efficiency and effectiveness in this specific domain.

\begin{equation}
[\hat{\bz}_k^{\MS}, \hat{\bz}_k^i] = \mathsf{TransformerEncoder} \left( \tilde{\bz}_k^{\MS}, \tilde{\bbf}_t, \tilde{\bz}_k^i, \tilde{\bc}_t^i, \tilde{\bu}_j \right) \, .
\end{equation}
We jointly encode all tokens spanning $\{k\}_1^K, \{i\}_1^N, \{t\}_1^T$, and $\{j\}_1^M$.

\paragraph{Emotion labeling.}
The contextualized representations for the scene $\hat{\bz}_k^{\MS}$ and characters $\hat{\bz}_k^i$ are sent to a shared linear layer $\bW^E \in \bbR^{K \times D}$ for classification.
Finally, the probability estimates through a sigmoid activation $\sigma(\cdot)$ are:
\begin{equation}
\label{eq:predictions}
\hat{y}_k^{\MS} = \sigma( \bW^E_k \hat{\bz}_k^{\MS} ) \,\,
\text{ and } \,\,
\hat{y}_k^i = \sigma( \bW^E_k \hat{\bz}_k^i ), \,\, \forall k, i \, .
\end{equation}
