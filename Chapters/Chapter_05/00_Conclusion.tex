\chapter{Conclusion}
\label{ch:conclusion}
In this thesis, we presented a novel task for multi-label emotion and mental state recognition at the level of a movie scene and for each character.

Our work, EmoTx~\cite{dhruv2023emotx}, a Transformer encoder based model, introduces a unique dimension by aiming to predict character-level mental states in addition to emotions and obtained significant improvements over previous works adapted for this task. 

Operating within the temporal framework of movie scenes, we demonstrate the influence of video and dialog context in enhancing the accuracy of predictions for these nuanced labels. This aspect differentiates our research from previous works and aligns with our goal of capturing the rich interplay of emotions and mental states within the context of cinematic storytelling.

Our learned model was shown to have interpretable attention scores across modalities -- they focused on the video or dialog context for mental states while looking at characters for expressive emotions.
In the future, \modelname{} may benefit from audio features or by considering the larger context of the movies instead of treating every scene independently.

The evolution of movie understanding has brought forth a diverse array of tasks and challenges, with each facet contributing to a more holistic comprehension of the cinematic medium. Our exploration, anchored in emotional and mental state identification, represents a valuable contribution to the multifaceted journey of understanding movies in all their narrative richness.
